{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from control.abstract_control import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NnA2CController(nn.Module, Controller):\n",
    "    def __init__(self, frames_shape):\n",
    "        \"\"\"Setus up all of the neural networks necessary for the controller to work\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frames_shape\n",
    "            the shape of the multidimensional carla data fetched from an agent's sensor\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        conv_net: nn.Sequential\n",
    "            a neural network responsible for extracting the current state of the environment, whose output is meant to serve as input for policy and critic networks\n",
    "        actor_net: nn.Sequential\n",
    "            a neural network responsible for the current agent's policy\n",
    "        critic_net: nn.Sequential\n",
    "            a neural network responsible for approximating the advantage function regarding the action space\n",
    "        \n",
    "        Methods\n",
    "        -------\n",
    "        TBD\n",
    "        \"\"\"\n",
    "        super(NnA2CController, self).__init__()\n",
    "        \n",
    "        #lenet inspired net upscaled due to carla frames being bigger than minst digits ;)\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=frames_shape[0], out_channels=128, kernel_size=7, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=4),\n",
    "            nn.Conv2d(in_channels=64, out_channels=120, kernel_size=3, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.conv_out_size = int(np.prod(self.conv_net(torch.zeros(1, *frames_shape)).size()))\n",
    "        \n",
    "        self.actor_net = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2) #2 returned values being the action taken, which consists of the gas/break pedal and steering angle suggested for steering\n",
    "        )\n",
    "\n",
    "        self.critic_net = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, sensor_data):\n",
    "        conv_output = self.conv_net(sensor_data).view(sensor_data.size()[0], -1)\n",
    "        return self.actor_net(conv_output), self.critic_net(conv_output)\n",
    "    \n",
    "    def control(self, state):\n",
    "\n",
    "        #leaving the following state elements here, but intend to use just camera data for starters\n",
    "        location = state['location']\n",
    "        x, y = location[0], location[1]\n",
    "        v = state['velocity'] # km / h #how to get speed?????????\n",
    "        Ïˆ = np.radians(state['yaw']) #adding 180 as carla returns yaw degrees in (-180, 180) range\n",
    "\n",
    "        actor_out, critic_out = self.forward(torch.cat((state['depth'], state['rgb']), 1))\n",
    "        \n",
    "        actions = {\n",
    "            'steer': actor_out[0][0],\n",
    "            'gas_brake':  actor_out[0][1],\n",
    "        }\n",
    "        \n",
    "        advantage = critic_out[0][0]\n",
    "\n",
    "        return actions, advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_controller = NnA2CController([8,75,100])\n",
    "nn_controller.conv_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': (1, 33),\n",
       " 'velocity': 5,\n",
       " 'yaw': 12,\n",
       " 'depth': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'rgb': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]])}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {'location': (1, 33), 'velocity': 5, 'yaw': 12, 'depth': torch.zeros(1, 4, 75, 100), 'rgb': torch.zeros(1, 4, 75, 100)}\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'steer': tensor(0.0530, grad_fn=<SelectBackward>),\n",
       "  'gas_brake': tensor(-0.0371, grad_fn=<SelectBackward>)},\n",
       " tensor(0.0406, grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nn_controller.control(state)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NnA2CController(\n",
       "  (conv_net): Sequential(\n",
       "    (0): Conv2d(8, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    (6): Conv2d(64, 120, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (actor_net): Sequential(\n",
       "    (0): Linear(in_features=4800, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (critic_net): Sequential(\n",
       "    (0): Linear(in_features=4800, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
